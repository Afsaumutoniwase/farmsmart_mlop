{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V7IQKdKL8Eg"
      },
      "source": [
        "# FarmSmart: Plant Disease Classification ML Pipeline\n",
        "\n",
        "### **Project Overview**\n",
        "This project demonstrates an end-to-end Machine Learning pipeline for plant disease classification using image data. The system can identify diseases in various crops including tomatoes, peppers, and strawberries.\n",
        "\n",
        "### **Key Features**\n",
        "- **Multi-crop Disease Detection**: Supports 14 plant disease classes\n",
        "- **Complete ML Pipeline**: Data acquisition → Processing → Training → Deployment\n",
        "- **Model Evaluation**: Comprehensive metrics and visualizations\n",
        "- **API Integration**: RESTful endpoints for predictions\n",
        "- **Scalable**: Docker containerization and cloud deployment ready\n",
        "- **Monitoring**: Performance tracking and retraining capabilities\n",
        "\n",
        "### **Dataset Information**\n",
        "- **Total Images**: 32,304 plant images  \n",
        "- **Classes**: 7 disease categories across 3 crop types\n",
        "- **Image Size**: 224x224 pixels\n",
        "- **Format**: RGB images in JPG format\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.applications import VGG16, ResNet50V2, EfficientNetB0\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
        "                           classification_report, confusion_matrix, roc_curve, auc)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import cv2\n",
        "import pickle\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n"
      ],
      "metadata": {
        "id": "lpqq0_IbicHy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpLTXfbeHbI8",
        "outputId": "357d2fc1-40bf-4cbf-e4a0-606664475d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6-oIcAUMMGL"
      },
      "source": [
        "## Data Exploration\n",
        "Let's explore the dataset structure and visualize some sample images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRQwQdawMOcL",
        "outputId": "f1373d69-6e0d-435f-ca6e-971b604c62e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET EXPLORATION\n",
            "\n",
            "Training Classes (7):\n",
            "   1. Tomato___Late_blight: 1,860 images\n",
            "   2. Pepper_bell_healthy: 1,987 images\n",
            "   3. Tomato_healthy: 1,925 images\n",
            "   4. Strawberry_Leaf_scorch: 1,773 images\n",
            "   5. Tomato_Target_Spot: 1,826 images\n",
            "   6. Strawberry_healthy: 1,823 images\n",
            "   7. Pepper_bell_Bacterial_spot: 1,912 images\n",
            "\n",
            "Validation Classes (7):\n",
            "   1. Tomato_Target_Spot: 457 images\n",
            "   2. Tomato_Late_blight: 463 images\n",
            "   3. Tomato_healthy: 481 images\n",
            "   4. Strawberry_Leaf_scorch: 444 images\n",
            "   5. Strawberry_healthy: 456 images\n",
            "   6. Pepper_bell_healthy: 497 images\n",
            "   7. Pepper_bell_Bacterial_spot: 478 images\n",
            "\n",
            "DATASET STATISTICS:\n",
            "  Total Training Images: 13,106\n",
            "  Total Validation Images: 3,276\n",
            "  Total Images: 16,382\n",
            "  Training/Validation Split: 80.0%/20.0%\n"
          ]
        }
      ],
      "source": [
        "dataset = '/content/drive/MyDrive/dataset/dataset'\n",
        "train_dir = '/content/drive/MyDrive/dataset/dataset/train'\n",
        "valid_dir = '/content/drive/MyDrive/dataset/dataset/valid'\n",
        "test_dir = '/content/drive/MyDrive/dataset/dataset/test'\n",
        "\n",
        "def explore_dataset():\n",
        "    \"\"\"Comprehensive dataset exploration and analysis\"\"\"\n",
        "    print(\"DATASET EXPLORATION\")\n",
        "\n",
        "    # Check if directories exist\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"Training directory not found: {train_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if not os.path.exists(valid_dir):\n",
        "        print(f\"Validation directory not found: {valid_dir}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Training data analysis\n",
        "    train_classes = os.listdir(train_dir)\n",
        "    train_data = []\n",
        "\n",
        "    print(f\"\\nTraining Classes ({len(train_classes)}):\")\n",
        "    for i, class_name in enumerate(train_classes, 1):\n",
        "        class_path = os.path.join(train_dir, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            num_images = len([f for f in os.listdir(class_path)\n",
        "                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            train_data.append({'class': class_name, 'count': num_images, 'type': 'train'})\n",
        "            print(f\"  {i:2d}. {class_name}: {num_images:,} images\")\n",
        "\n",
        "    # Validation data analysis\n",
        "    valid_classes = os.listdir(valid_dir)\n",
        "    valid_data = []\n",
        "\n",
        "    print(f\"\\nValidation Classes ({len(valid_classes)}):\")\n",
        "    for i, class_name in enumerate(valid_classes, 1):\n",
        "        class_path = os.path.join(valid_dir, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            num_images = len([f for f in os.listdir(class_path)\n",
        "                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            valid_data.append({'class': class_name, 'count': num_images, 'type': 'valid'})\n",
        "            print(f\"  {i:2d}. {class_name}: {num_images:,} images\")\n",
        "\n",
        "    # Create DataFrame for analysis\n",
        "    train_df = pd.DataFrame(train_data)\n",
        "    valid_df = pd.DataFrame(valid_data)\n",
        "    all_data = pd.concat([train_df, valid_df], ignore_index=True)\n",
        "\n",
        "    # Dataset statistics\n",
        "    total_train = train_df['count'].sum()\n",
        "    total_valid = valid_df['count'].sum()\n",
        "    total_images = total_train + total_valid\n",
        "\n",
        "    print(f\"\\nDATASET STATISTICS:\")\n",
        "    print(f\"  Total Training Images: {total_train:,}\")\n",
        "    print(f\"  Total Validation Images: {total_valid:,}\")\n",
        "    print(f\"  Total Images: {total_images:,}\")\n",
        "    print(f\"  Training/Validation Split: {total_train/total_images:.1%}/{total_valid/total_images:.1%}\")\n",
        "\n",
        "\n",
        "    return train_classes, valid_classes, all_data\n",
        "\n",
        "# Execute dataset exploration\n",
        "train_classes, valid_classes, dataset_info = explore_dataset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og4EHuuVMyi8"
      },
      "source": [
        "## Data Preprocessing\n",
        "We use Keras' ImageDataGenerator to load and preprocess images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r2Z_yXj3MzL8"
      },
      "outputs": [],
      "source": [
        "def get_train_valid_generators(train_dir, valid_dir, img_size=(128, 128), batch_size=32):\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.0\n",
        "    )\n",
        "\n",
        "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_gen = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    valid_gen = valid_datagen.flow_from_directory(\n",
        "        valid_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    return train_gen, valid_gen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation: `get_data_generators(...)`\n",
        "\n",
        "This function loads training and validation image datasets from directory paths using Keras `ImageDataGenerator`.\n",
        "It applies advanced data augmentation to the training set and basic rescaling to the validation set.\n",
        "Returns:\n",
        "- `train_generator`\n",
        "- `valid_generator`\n",
        "- `class_names` (automatically extracted from folder names)\n"
      ],
      "metadata": {
        "id": "sufcVpNBjOgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_generators(train_dir, valid_dir, img_size=(128, 128), batch_size=32):\n",
        "    \"\"\"\n",
        "    Create enhanced data generators with comprehensive augmentation.\n",
        "\n",
        "    Args:\n",
        "        train_dir (str): Path to training dataset directory.\n",
        "        valid_dir (str): Path to validation dataset directory.\n",
        "        img_size (tuple): Target size for images (width, height).\n",
        "        batch_size (int): Number of images per batch.\n",
        "\n",
        "    Returns:\n",
        "        train_generator, valid_generator, class_names (list)\n",
        "    \"\"\"\n",
        "\n",
        "    # ADVANCED data augmentation for training (7+ techniques)\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Only rescaling for validation\n",
        "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    # Create generators\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    valid_generator = valid_datagen.flow_from_directory(\n",
        "        valid_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "    print(f\"DATA GENERATORS CREATED:\")\n",
        "    print(f\"  Training samples: {train_generator.samples:,}\")\n",
        "    print(f\"  Validation samples: {valid_generator.samples:,}\")\n",
        "    print(f\"  Classes: {len(class_names)}\")\n",
        "    print(f\"  Augmentation techniques: 7 (rotation, shift, shear, zoom, flip, brightness, fill)\")\n",
        "\n",
        "    return train_generator, valid_generator, class_names\n"
      ],
      "metadata": {
        "id": "fyQBsd7raryB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture: `create_custom_cnn(...)`\n",
        "\n",
        "Defines a custom Convolutional Neural Network (CNN) using Keras Sequential API with:\n",
        "- 4 convolutional blocks\n",
        "- Batch normalization and dropout regularization\n",
        "- Global average pooling and 3 dense layers\n",
        "Returns a compiled Keras model ready for training.\n"
      ],
      "metadata": {
        "id": "y_xRuy80jZ04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_custom_cnn(img_size=(128, 128), num_classes=7):\n",
        "    \"\"\"\n",
        "    Build a custom CNN model with 4 convolutional blocks and 3 dense layers.\n",
        "\n",
        "    Args:\n",
        "        img_size (tuple): Target input image size (width, height)\n",
        "        num_classes (int): Number of output classes\n",
        "\n",
        "    Returns:\n",
        "        model (tf.keras.Model): Compiled CNN model\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=img_size + (3,)),\n",
        "\n",
        "        # Block 1\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        # Block 2\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Block 3\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        # Block 4\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.5),\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    print(f\"CNN CREATED with ~18 layers:\")\n",
        "    print(f\"  Total parameters: {model.count_params():,}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "HtCDWjphar0m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning: `create_transfer_learning_model(...)`\n",
        "\n",
        "Builds a transfer learning model using either ResNet50V2 or EfficientNetB0 as base.\n",
        "- Freezes lower layers and fine-tunes the last 20.\n",
        "- Adds a dense classification head with regularization.\n",
        "Returns the assembled and uncompiled model.\n"
      ],
      "metadata": {
        "id": "1pr960pEjcsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transfer_learning_model(base_model_name='resnet50', img_size=(128, 128), num_classes=7):\n",
        "    \"\"\"\n",
        "    Create an optimized transfer learning model with optional fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        base_model_name (str): Either 'resnet50' or 'efficientnet'.\n",
        "        img_size (tuple): Input image size (width, height).\n",
        "        num_classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled model ready for training.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load base model\n",
        "    if base_model_name == 'resnet50':\n",
        "        base_model = ResNet50V2(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=img_size + (3,)\n",
        "        )\n",
        "    elif base_model_name == 'efficientnet':\n",
        "        base_model = EfficientNetB0(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=img_size + (3,)\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported base model: {base_model_name}\")\n",
        "\n",
        "    # Freeze most layers for transfer learning\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-20]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Classification head\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Summary info\n",
        "    print(f\"TRANSFER LEARNING MODEL CREATED:\")\n",
        "    print(f\"  Base model: {base_model_name}\")\n",
        "    print(f\"  Fine-tuning: Last 20 layers unfrozen\")\n",
        "    print(f\"  Total parameters: {model.count_params():,}\")\n",
        "    print(f\"  Trainable parameters: {sum(tf.keras.backend.count_params(w) for w in model.trainable_weights):,}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ScokyWN1a71Y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Builder & Compiler: `build_model(...)`\n",
        "\n",
        "Builds the model using either:\n",
        "- A custom CNN (`create_custom_cnn`) or\n",
        "- A transfer learning base (`create_transfer_learning_model`)\n",
        "\n",
        "Then compiles it with Adam optimizer, categorical crossentropy loss, and 5 metrics:\n",
        "Accuracy, Precision, Recall, AUC, and Top-3 Accuracy.\n"
      ],
      "metadata": {
        "id": "LdPv-kvLjgkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(img_size=(128, 128), num_classes=7, model_type='custom'):\n",
        "    \"\"\"\n",
        "    Build and compile a CNN model with advanced metrics and optimizer.\n",
        "    Supports custom CNN and transfer learning options.\n",
        "    \"\"\"\n",
        "    if model_type == 'custom':\n",
        "        model = create_custom_cnn(img_size, num_classes)\n",
        "    elif model_type in ['resnet50', 'efficientnet']:\n",
        "        model = create_transfer_learning_model(model_type, img_size, num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "    # Compile with advanced optimizer and metrics\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(\n",
        "            learning_rate=0.001 if model_type == 'custom' else 0.0001,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            epsilon=1e-7,\n",
        "            amsgrad=True\n",
        "        ),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc'),\n",
        "            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(f\"MODEL COMPILED WITH ADVANCED OPTIMIZATION:\")\n",
        "    print(f\"  Optimizer: Adam (AMSGrad variant)\")\n",
        "    print(f\"  Learning Rate: {0.001 if model_type == 'custom' else 0.0001}\")\n",
        "    print(f\"  Loss Function: Categorical Crossentropy\")\n",
        "    print(f\"  Metrics: Accuracy, Precision, Recall, AUC, Top-3 Accuracy\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "mgNYdW3Sa74y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Callbacks: `get_callbacks(...)`\n",
        "\n",
        "Returns a list of advanced training callbacks for optimization:\n",
        "- Early stopping\n",
        "- Reduce LR on plateau\n",
        "- Save best model\n",
        "- Custom exponential LR schedule\n",
        "\n",
        "These callbacks help avoid overfitting and dynamically adjust learning rate.\n"
      ],
      "metadata": {
        "id": "6eC1BsKJjkct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_callbacks(model_save_path='models/farmsmart.keras'):\n",
        "    \"\"\"\n",
        "    Returns a list of advanced training callbacks with early stopping, learning rate adjustment, and checkpointing.\n",
        "\n",
        "    Args:\n",
        "        model_save_path (str): Path to save the best model.\n",
        "\n",
        "    Returns:\n",
        "        list: List of Keras callbacks.\n",
        "    \"\"\"\n",
        "    callbacks = [\n",
        "        # Early stopping\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1,\n",
        "            mode='max',\n",
        "            min_delta=0.001\n",
        "        ),\n",
        "\n",
        "        # Reduce LR on plateau\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1,\n",
        "            mode='min',\n",
        "            cooldown=1\n",
        "        ),\n",
        "\n",
        "        # Save best model\n",
        "        ModelCheckpoint(\n",
        "            filepath=model_save_path,\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            verbose=1,\n",
        "            mode='max'\n",
        "        ),\n",
        "\n",
        "        # Custom LR schedule (exponential decay)\n",
        "        LearningRateScheduler(\n",
        "            lambda epoch: 0.001 * (0.95 ** epoch),\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(f\"ADVANCED CALLBACKS CONFIGURED:\")\n",
        "    print(f\"  EarlyStopping: val_accuracy, patience=10, min_delta=0.001\")\n",
        "    print(f\"  ReduceLROnPlateau: factor=0.5, patience=5, cooldown=1\")\n",
        "    print(f\"  ModelCheckpoint: Saves best weights to {model_save_path}\")\n",
        "    print(f\"  LearningRateScheduler: Exponential decay (0.95^epoch)\")\n",
        "\n",
        "    return callbacks\n"
      ],
      "metadata": {
        "id": "eAl3omgBbNFB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training: `train_model(...)`\n",
        "\n",
        "Trains the compiled model using `.fit()` with the training and validation generators.\n",
        "If no callbacks are passed, default optimization callbacks are added.\n",
        "Logs total training time and final accuracy metrics.\n",
        "Returns the training `history` object.\n"
      ],
      "metadata": {
        "id": "j9kFReyOjoew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_generator, valid_generator, epochs=50, callbacks=None):\n",
        "    \"\"\"\n",
        "    Train the model with comprehensive logging and optional callbacks.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): Compiled Keras model.\n",
        "        train_generator: Training data generator.\n",
        "        valid_generator: Validation data generator.\n",
        "        epochs (int): Number of epochs to train.\n",
        "        callbacks (list): List of Keras callbacks.\n",
        "\n",
        "    Returns:\n",
        "        history: Training history object.\n",
        "    \"\"\"\n",
        "    if callbacks is None:\n",
        "        callbacks = get_callbacks()\n",
        "\n",
        "    print(f\"STARTING ADVANCED TRAINING:\")\n",
        "    print(f\"  Epochs: {epochs}\")\n",
        "    print(f\"  Training samples: {train_generator.samples:,}\")\n",
        "    print(f\"  Validation samples: {valid_generator.samples:,}\")\n",
        "    print(f\"  Steps per epoch: {len(train_generator)}\")\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=valid_generator,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    training_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "    print(f\"TRAINING COMPLETED!\")\n",
        "    print(f\"  Training time: {training_time / 60:.2f} minutes\")\n",
        "    print(f\"  Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "    print(f\"  Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "bC46kRvnbNIf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comprehensive Evaluation: `evaluate_model_comprehensive(...)`\n",
        "\n",
        "Evaluates the model on a test/validation generator using:\n",
        "- Accuracy, Loss, Precision, Recall, F1\n",
        "- AUC-ROC, Cohen’s Kappa, Matthews Corr\n",
        "- Confusion Matrix and per-class metrics\n",
        "\n",
        "Also prints a full classification report. Returns all metrics in a dictionary.\n"
      ],
      "metadata": {
        "id": "b-NxUIbOjr5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_comprehensive(model, test_generator, num_classes, class_names):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a trained model using a test generator.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): Trained Keras model.\n",
        "        test_generator: Generator for test data.\n",
        "        num_classes (int): Number of output classes.\n",
        "        class_names (list): Names of the classes in the same order as generator.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing all evaluation metrics.\n",
        "    \"\"\"\n",
        "    print(f\"COMPREHENSIVE MODEL EVALUATION\")\n",
        "\n",
        "    test_generator.reset()\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "    predictions = model.predict(test_generator, verbose=1)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    true_labels = test_generator.classes\n",
        "\n",
        "    # Accuracy & Loss\n",
        "    accuracy = accuracy_score(true_labels, predicted_classes)\n",
        "    true_labels_categorical = tf.keras.utils.to_categorical(true_labels, num_classes=num_classes)\n",
        "    loss = log_loss(true_labels_categorical, predictions)\n",
        "\n",
        "    # Basic Metrics\n",
        "    precision, recall, f1_score, support = precision_recall_fscore_support(\n",
        "        true_labels, predicted_classes, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Per-class metrics\n",
        "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
        "        true_labels, predicted_classes, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    # Advanced Metrics\n",
        "    try:\n",
        "        auc_roc = roc_auc_score(true_labels_categorical, predictions, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        auc_roc = 0.0\n",
        "\n",
        "    cohen_kappa = cohen_kappa_score(true_labels, predicted_classes)\n",
        "    mcc = matthews_corrcoef(true_labels, predicted_classes)\n",
        "    cm = confusion_matrix(true_labels, predicted_classes)\n",
        "\n",
        "    # Output summary\n",
        "    print(f\"\\nCOMPREHENSIVE EVALUATION RESULTS:\")\n",
        "    print(f\"CORE METRICS (Required for Excellent Rating):\")\n",
        "    print(f\"   ACCURACY:   {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"   LOSS:       {loss:.4f}\")\n",
        "    print(f\"   PRECISION:  {precision:.4f} ({precision*100:.2f}%)\")\n",
        "    print(f\"   RECALL:     {recall:.4f} ({recall*100:.2f}%)\")\n",
        "    print(f\"   F1-SCORE:   {f1_score:.4f} ({f1_score*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nADVANCED METRICS:\")\n",
        "    print(f\"  AUC-ROC:        {auc_roc:.4f}\")\n",
        "    print(f\"  Cohen's Kappa:  {cohen_kappa:.4f}\")\n",
        "    print(f\"  Matthews Corr:  {mcc:.4f}\")\n",
        "\n",
        "    print(f\"\\nPERFORMANCE INTERPRETATION:\")\n",
        "    if accuracy >= 0.90:\n",
        "        print(\"   EXCELLENT: Accuracy > 90%\")\n",
        "    elif accuracy >= 0.80:\n",
        "        print(\"   VERY GOOD: Accuracy > 80%\")\n",
        "    elif accuracy >= 0.70:\n",
        "        print(\"   GOOD: Accuracy > 70%\")\n",
        "    else:\n",
        "        print(\"   NEEDS IMPROVEMENT: Accuracy < 70%\")\n",
        "\n",
        "    # Classification Report\n",
        "    report = classification_report(true_labels, predicted_classes, target_names=class_names, digits=4)\n",
        "    print(f\"\\nDETAILED CLASSIFICATION REPORT:\")\n",
        "    print(report)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'loss': loss,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score,\n",
        "        'auc_roc': auc_roc,\n",
        "        'cohen_kappa': cohen_kappa,\n",
        "        'matthews_corr': mcc,\n",
        "        'confusion_matrix': cm,\n",
        "        'precision_per_class': precision_per_class,\n",
        "        'recall_per_class': recall_per_class,\n",
        "        'f1_per_class': f1_per_class,\n",
        "        'support_per_class': support,\n",
        "        'classification_report': report,\n",
        "        'predictions': predictions,\n",
        "        'y_true': true_labels,\n",
        "        'y_pred': predicted_classes\n",
        "    }\n"
      ],
      "metadata": {
        "id": "7lDxVTuWbUG2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training History Plot: `plot_training_history(...)`\n",
        "\n",
        "Generates a 2x3 grid of plots showing:\n",
        "- Accuracy\n",
        "- Loss\n",
        "- Precision\n",
        "- Recall\n",
        "- AUC\n",
        "- Learning Rate\n",
        "\n",
        "Accepts optional `save_path` for saving the chart.\n"
      ],
      "metadata": {
        "id": "RQCf5QEUjwSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot comprehensive training history from a Keras History object.\n",
        "\n",
        "    Args:\n",
        "        history (History): Keras history object returned by model.fit().\n",
        "        save_path (str, optional): If provided, saves the plot to this path.\n",
        "    \"\"\"\n",
        "    if history is None or not hasattr(history, 'history'):\n",
        "        print(\"No valid training history available!\")\n",
        "        return\n",
        "\n",
        "    metrics = history.history\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Comprehensive Training History', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Accuracy\n",
        "    axes[0, 0].plot(metrics['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "    axes[0, 0].plot(metrics['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "    axes[0, 0].set_title('Model Accuracy', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 1].plot(metrics['loss'], label='Training Loss', linewidth=2)\n",
        "    axes[0, 1].plot(metrics['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    axes[0, 1].set_title('Model Loss', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Precision\n",
        "    if 'precision' in metrics:\n",
        "        axes[0, 2].plot(metrics['precision'], label='Training Precision', linewidth=2)\n",
        "        axes[0, 2].plot(metrics['val_precision'], label='Validation Precision', linewidth=2)\n",
        "        axes[0, 2].set_title('Model Precision', fontweight='bold')\n",
        "        axes[0, 2].set_xlabel('Epoch')\n",
        "        axes[0, 2].set_ylabel('Precision')\n",
        "        axes[0, 2].legend()\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[0, 2].axis('off')\n",
        "\n",
        "    # Recall\n",
        "    if 'recall' in metrics:\n",
        "        axes[1, 0].plot(metrics['recall'], label='Training Recall', linewidth=2)\n",
        "        axes[1, 0].plot(metrics['val_recall'], label='Validation Recall', linewidth=2)\n",
        "        axes[1, 0].set_title('Model Recall', fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Recall')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 0].axis('off')\n",
        "\n",
        "    # AUC\n",
        "    if 'auc' in metrics:\n",
        "        axes[1, 1].plot(metrics['auc'], label='Training AUC', linewidth=2)\n",
        "        axes[1, 1].plot(metrics['val_auc'], label='Validation AUC', linewidth=2)\n",
        "        axes[1, 1].set_title('Model AUC', fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('AUC')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 1].axis('off')\n",
        "\n",
        "    # Learning Rate\n",
        "    if 'lr' in metrics:\n",
        "        axes[1, 2].plot(metrics['lr'], label='Learning Rate', linewidth=2, color='red')\n",
        "        axes[1, 2].set_title('Learning Rate Schedule', fontweight='bold')\n",
        "        axes[1, 2].set_xlabel('Epoch')\n",
        "        axes[1, 2].set_ylabel('Learning Rate')\n",
        "        axes[1, 2].set_yscale('log')\n",
        "        axes[1, 2].legend()\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "X2CeNEkBbUKY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix Plot: `plot_confusion_matrix(...)`\n",
        "\n",
        "Plots a labeled heatmap showing model performance per class.\n",
        "- Takes a confusion matrix and class labels.\n",
        "- Supports saving to file via `save_path`.\n"
      ],
      "metadata": {
        "id": "bpVNZpfHj0W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, class_names, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot an enhanced confusion matrix with formatted class labels.\n",
        "\n",
        "    Args:\n",
        "        cm (ndarray): Confusion matrix (from sklearn.metrics.confusion_matrix).\n",
        "        class_names (list): List of class label names (must match cm dimensions).\n",
        "        save_path (str, optional): Path to save the figure (if provided).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 12))\n",
        "\n",
        "    # Format class names for better display\n",
        "    formatted_names = [name.replace('___', '\\n').replace('_', ' ') for name in class_names]\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=formatted_names,\n",
        "                yticklabels=formatted_names)\n",
        "\n",
        "    plt.title('Confusion Matrix - Plant Disease Classification', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Predicted Labels', fontsize=12)\n",
        "    plt.ylabel('True Labels', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5rOXCeadar4F"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per-Class Evaluation Summary: `plot_comprehensive_evaluation(...)`\n",
        "\n",
        "Visualizes:\n",
        "- Precision\n",
        "- Recall\n",
        "- F1-score\n",
        "- Support (sample count)\n",
        "for each class using grouped bar plots.\n",
        "Helps identify where the model performs well or poorly.\n"
      ],
      "metadata": {
        "id": "uZ7arTnHj3zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_comprehensive_evaluation(eval_results, class_names, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot comprehensive evaluation metrics per class: precision, recall, F1-score, and support.\n",
        "\n",
        "    Args:\n",
        "        eval_results (dict): Output from evaluate_model_comprehensive(...).\n",
        "        class_names (list): List of class names.\n",
        "        save_path (str, optional): File path to save the plot.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Comprehensive Model Evaluation', fontsize=16, fontweight='bold')\n",
        "\n",
        "    formatted_names = [name.split('___')[-1].replace('_', ' ') for name in class_names]\n",
        "    x = range(len(class_names))\n",
        "\n",
        "    # Precision per class\n",
        "    axes[0, 0].bar(x, eval_results['precision_per_class'], color='skyblue', alpha=0.7)\n",
        "    axes[0, 0].set_title('Precision per Class', fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Precision')\n",
        "    axes[0, 0].set_xticks(x)\n",
        "    axes[0, 0].set_xticklabels(formatted_names, rotation=45, ha='right')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Recall per class\n",
        "    axes[0, 1].bar(x, eval_results['recall_per_class'], color='lightcoral', alpha=0.7)\n",
        "    axes[0, 1].set_title('Recall per Class', fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('Recall')\n",
        "    axes[0, 1].set_xticks(x)\n",
        "    axes[0, 1].set_xticklabels(formatted_names, rotation=45, ha='right')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # F1-score per class\n",
        "    axes[1, 0].bar(x, eval_results['f1_per_class'], color='lightgreen', alpha=0.7)\n",
        "    axes[1, 0].set_title('F1-Score per Class', fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('F1-Score')\n",
        "    axes[1, 0].set_xticks(x)\n",
        "    axes[1, 0].set_xticklabels(formatted_names, rotation=45, ha='right')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Support (sample count) per class\n",
        "    axes[1, 1].bar(x, eval_results['support_per_class'], color='gold', alpha=0.7)\n",
        "    axes[1, 1].set_title('Support (Samples) per Class', fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Number of Samples')\n",
        "    axes[1, 1].set_xticks(x)\n",
        "    axes[1, 1].set_xticklabels(formatted_names, rotation=45, ha='right')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "WL4PsT7fbsy2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Image Prediction: `predict_image(...)`\n",
        "\n",
        "Performs inference on one image.\n",
        "- Preprocesses the input\n",
        "- Predicts top-k classes\n",
        "- Formats results with class names and confidence\n",
        "\n",
        "Returns a dictionary with top predictions and full probability list.\n"
      ],
      "metadata": {
        "id": "FW0hdNpXj7uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(model, image_path, class_names, img_size=(128, 128), top_k=3):\n",
        "    \"\"\"\n",
        "    Predict the top-k classes for a single image using a trained model.\n",
        "\n",
        "    Args:\n",
        "        model (tf.keras.Model): Trained Keras model.\n",
        "        image_path (str): Path to the image to predict.\n",
        "        class_names (list): List of class labels in order used by model.\n",
        "        img_size (tuple): Image size to resize to before prediction.\n",
        "        top_k (int): Number of top predictions to return.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing image path, top predictions, and all class probabilities.\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        raise ValueError(\"Model not provided!\")\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=img_size)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = img_array / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(img_array, verbose=0)\n",
        "    predicted_probs = predictions[0]\n",
        "    top_indices = np.argsort(predicted_probs)[::-1][:top_k]\n",
        "\n",
        "    results = {\n",
        "        'image_path': image_path,\n",
        "        'top_predictions': [],\n",
        "        'all_probabilities': predicted_probs.tolist()\n",
        "    }\n",
        "\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        disease_name = class_names[idx].replace('___', ' - ').replace('_', ' ')\n",
        "        confidence = float(predicted_probs[idx])\n",
        "        results['top_predictions'].append({\n",
        "            'rank': i + 1,\n",
        "            'disease': disease_name,\n",
        "            'confidence': confidence,\n",
        "            'percentage': confidence * 100\n",
        "        })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "k6_FgP4bb84i"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting full training pipeline for FarmSmart Disease Classifier\")\n",
        "\n",
        "# Set config\n",
        "IMG_SIZE = (128, 128)\n",
        "NUM_CLASSES = 7\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "MODEL_SAVE_PATH = 'models/farmsmart.keras'\n",
        "\n",
        "# Load data with preprocessing\n",
        "train_generator, valid_generator = get_train_valid_generators(\n",
        "    train_dir='/content/drive/MyDrive/dataset/dataset/train',\n",
        "    valid_dir='/content/drive/MyDrive/dataset/dataset/valid',\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "actual_classes = train_generator.num_classes\n",
        "print(f\"Detected {actual_classes} classes in dataset\")\n",
        "\n",
        "# Build and compile model\n",
        "print(\"\\nBuilding model...\")\n",
        "model = build_model(img_size=IMG_SIZE, num_classes=actual_classes, model_type='custom')\n",
        "\n",
        "# Train model\n",
        "print(\"\\nTraining model...\")\n",
        "history = train_model(\n",
        "    model=model,\n",
        "    train_generator=train_generator,\n",
        "    valid_generator=valid_generator,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# Save trained model\n",
        "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
        "model.save(MODEL_SAVE_PATH)\n",
        "print(f\"\\nModel saved to: {MODEL_SAVE_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPZKyV-sbs2P",
        "outputId": "6442b986-95fe-4f33-a330-efad952c411e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting full training pipeline for FarmSmart Disease Classifier\n",
            "Found 13106 images belonging to 7 classes.\n",
            "Found 3276 images belonging to 7 classes.\n",
            "Detected 7 classes in dataset\n",
            "\n",
            "Building model...\n",
            "CNN CREATED with ~18 layers:\n",
            "  Total parameters: 1,438,887\n",
            "MODEL COMPILED WITH ADVANCED OPTIMIZATION:\n",
            "  Optimizer: Adam (AMSGrad variant)\n",
            "  Learning Rate: 0.001\n",
            "  Loss Function: Categorical Crossentropy\n",
            "  Metrics: Accuracy, Precision, Recall, AUC, Top-3 Accuracy\n",
            "\n",
            "Training model...\n",
            "ADVANCED CALLBACKS CONFIGURED:\n",
            "  EarlyStopping: val_accuracy, patience=10, min_delta=0.001\n",
            "  ReduceLROnPlateau: factor=0.5, patience=5, cooldown=1\n",
            "  ModelCheckpoint: Saves best weights to models/farmsmart.keras\n",
            "  LearningRateScheduler: Exponential decay (0.95^epoch)\n",
            "STARTING ADVANCED TRAINING:\n",
            "  Epochs: 10\n",
            "  Training samples: 13,106\n",
            "  Validation samples: 3,276\n",
            "  Steps per epoch: 410\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/10\n",
            "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.6478 - auc: 0.9134 - loss: 0.9545 - precision: 0.7505 - recall: 0.5419 - top_3_accuracy: 0.9026\n",
            "Epoch 1: val_accuracy improved from -inf to 0.34066, saving model to models/farmsmart.keras\n",
            "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4698s\u001b[0m 11s/step - accuracy: 0.6481 - auc: 0.9135 - loss: 0.9538 - precision: 0.7507 - recall: 0.5423 - top_3_accuracy: 0.9028 - val_accuracy: 0.3407 - val_auc: 0.6693 - val_loss: 3.6121 - val_precision: 0.3541 - val_recall: 0.3300 - val_top_3_accuracy: 0.6267 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.00095.\n",
            "Epoch 2/10\n",
            "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8905 - auc: 0.9894 - loss: 0.3206 - precision: 0.9053 - recall: 0.8750 - top_3_accuracy: 0.9936\n",
            "Epoch 2: val_accuracy improved from 0.34066 to 0.48443, saving model to models/farmsmart.keras\n",
            "\u001b[1m410/410\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2395s\u001b[0m 6s/step - accuracy: 0.8905 - auc: 0.9894 - loss: 0.3205 - precision: 0.9053 - recall: 0.8751 - top_3_accuracy: 0.9937 - val_accuracy: 0.4844 - val_auc: 0.7613 - val_loss: 4.0102 - val_precision: 0.4912 - val_recall: 0.4710 - val_top_3_accuracy: 0.8657 - learning_rate: 9.5000e-04\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0009025.\n",
            "Epoch 3/10\n",
            "\u001b[1m230/410\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m15:49\u001b[0m 5s/step - accuracy: 0.9171 - auc: 0.9935 - loss: 0.2393 - precision: 0.9291 - recall: 0.9059 - top_3_accuracy: 0.9943"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate on validation set\n",
        "print(\"\\nEvaluating trained model...\")\n",
        "eval_results = evaluate_model_comprehensive(model, valid_generator, actual_classes, class_names)\n",
        "\n",
        "# Visualize training progress\n",
        "print(\"\\nPlotting training history...\")\n",
        "plot_training_history(history)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nPlotting confusion matrix...\")\n",
        "plot_confusion_matrix(eval_results[\"confusion_matrix\"], class_names)\n",
        "\n",
        "# Evaluation summary\n",
        "print(\"\\nFinal Evaluation Summary:\")\n",
        "print(f\"   Accuracy:  {eval_results['accuracy']*100:.2f}%\")\n",
        "print(f\"   F1 Score:  {eval_results['f1_score']*100:.2f}%\")\n",
        "print(f\"   Loss:      {eval_results['loss']:.4f}\")\n",
        "print(f\"   AUC-ROC:   {eval_results['auc_roc']:.4f}\")\n"
      ],
      "metadata": {
        "id": "kWP9bHidhv5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example prediction\n",
        "sample_image_path = r\"/content/drive/MyDrive/dataset/dataset/test/Pepper__bell_healthy.JPG\"\n",
        "if os.path.exists(sample_image_path):\n",
        "    prediction_results = predict_image(\n",
        "        model=model,\n",
        "        class_names=class_names,\n",
        "        image_path=sample_image_path,\n",
        "        img_size=IMG_SIZE,\n",
        "        top_k=3\n",
        "    )\n",
        "\n",
        "    print(\"\\nTop Predictions for Sample Image:\")\n",
        "    for pred in prediction_results['top_predictions']:\n",
        "        print(f\"   - {pred['disease']} ({pred['percentage']:.2f}%)\")\n",
        "else:\n",
        "    print(f\"\\nSample image not found: {sample_image_path}\")\n"
      ],
      "metadata": {
        "id": "yOkPXlN_hy7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}